# E3 Project - CSDL-14B on DGX Spark

## Hardware Environment

You are working on an **NVIDIA DGX Spark** with:
- **GPU**: Grace Blackwell GB10
- **Memory**: 128GB unified memory
- **CPU**: 20 ARM cores
- **CUDA**: Version 13.0
- **Architecture**: 121a (Grace Blackwell)

## Project Context

This is the **E3 platform** featuring:
1. **CSDL-14B** - Fine-tuned 14B parameter LLM for Compressed Semantic Data Language
   - Base: Qwen2.5-14B-Instruct (14.7B parameters)
   - Training: 32,000+ CSDL compression examples
   - Format: SafeTensors F16
   - Compression: 90-98% token reduction

2. **E3-DevMind-AI** - 32-agent CSDL-native cognitive swarm
   - 32 specialized agents in functional groups
   - Pure CSDL protocol communication

## CRITICAL: llama.cpp Tokenization Bug

**llama.cpp has a tokenization bug on ARM64 + Blackwell architecture!**

- The model outputs garbage/Chinese characters when using llama.cpp
- This is NOT a model issue - the fine-tuning is correct
- **Solution**: Use the Python HuggingFace server on port 5000 instead
- Requires PyTorch built with CUDA from source (2-4 hour build)

See `CSDL-14B/docs/CSDL-MODEL-DIAGNOSTIC.md` for full investigation.

## Model Paths

- **Source models**: `/home/bodhifreeman/E3/model-training/model/merged_16bit/`
- **Converted GGUF**: `/home/bodhifreeman/E3/csdl-14b-f16.gguf` (for llama.cpp - HAS BUGS)
- **llama.cpp**: `/home/bodhifreeman/E3/llama.cpp/`
- **CSDL-ANLT Server**: `/home/bodhifreeman/E3/CSDL-ANLT/csdl-server.py`

## Service Ports

| Service | Port | Description | Status |
|---------|------|-------------|--------|
| **CSDL Server** | **5000** | **CSDL-14B inference (Python/HuggingFace) - PRIMARY** | **Working** |
| Llama Server | 8002 | llama.cpp (HAS TOKENIZATION BUGS on ARM64+Blackwell) | DO NOT USE |
| Archon API | 8181 | Archon REST API | Docker |
| Archon MCP | 8051 | Archon MCP Server | Docker |
| Archon UI | 3737 | Archon Web Interface | Docker |
| E3 AG-UI Server | 8100 | CopilotKit backend | Python/FastAPI |
| E3 DevMind UI | 3000 | CopilotKit frontend | Next.js |

**Note**: The Python CSDL server on port 5000 is the CORRECT backend. It uses HuggingFace Transformers with proper tokenization. The llama.cpp server on port 8002 has tokenization bugs on this hardware - DO NOT USE.

### Quick Port Check Command
```bash
for port in 5000 8100 8181 8051 3737 3000; do
  nc -z localhost $port 2>/dev/null && echo "Port $port: UP" || echo "Port $port: DOWN"
done
```

## PyTorch CUDA Build

PyTorch with CUDA must be built from source for ARM64 + Blackwell:

```bash
cd /home/bodhifreeman/E3/Scripts
./build-pytorch-cuda.sh
```

Build takes 2-4 hours. After build:
```bash
source /home/bodhifreeman/pytorch-build/pytorch-venv/bin/activate
pip install transformers accelerate fastapi uvicorn
```

Then update `CSDL-ANLT/csdl-server.py`:
- Change `device_map="cpu"` to `device_map="cuda:0"`
- Change `torch_dtype=torch.float32` to `torch_dtype=torch.bfloat16`

## CSDL Protocol (4-Layer Compression)

The model uses **Compressed Semantic Data Language** with 4 layers of compression:

| Layer | Technology | Compression | Description |
|-------|------------|-------------|-------------|
| Layer 1 | ANLT Translation | Semantic extraction | Natural language â†’ CSDL field codes |
| Layer 2 | Semantic Embeddings | 80-90% | Dense vector representations |
| Layer 3 | CBP Binary | 30-60% | MessagePack + LZ4 wire format |
| Layer 4 | Deduplication | **86%+** | xxHash content-addressed storage |

**CSDL Field Codes (single-char for max compression):**
- `T` = Type (q=query, c=cmd, r=result, x=error, h=handoff)
- `C` = Content (`{i: intent, k: keywords}`)
- `R` = Response format (b=brief, d=detailed, s=structured)
- `cx` = Context (scope, domain, temporal)
- `p` = Priority (0-3: low, normal, high, critical)
- `m` = Metadata (optional)

**Intent Codes:**
- `an`=analyze, `rk`=risk, `ds`=design, `im`=implement
- `ts`=test, `op`=optimize, `sc`=security, `qr`=query

**Example CSDL:**
```json
{"T":"q","C":{"i":"sc","k":["auth","risks"]},"cx":{"d":"auth"},"R":"d","p":1}
```

**Verified Compression Results:**
- Single message (CBP vs JSON): ~30% reduction
- Multi-hop pipeline (4 agents): ~46% reduction
- Repeated queries (10x same): **86% reduction**
- Dedup hit rate: 90%+

## Scripts Available (in /home/bodhifreeman/E3/Scripts/)

- `start-e3-full-stack.sh` - Start all services (CSDL, Archon, AG-UI, UI)
- `stop-e3-full-stack.sh` - Stop all services
- `start-e3-devmind-swarm.sh` - Start AG-UI + DevMind UI
- `build-pytorch-cuda.sh` - Build PyTorch with CUDA from source
- `convert-csdl-to-gguf.sh` - Convert SafeTensors to GGUF (for llama.cpp)

## Desktop Launchers

Located in `~/Desktop/`:
- **Start E3 Full Stack** - Starts CSDL + Archon + AG-UI + DevMind UI
- **Start E3 DevMind Swarm** - Starts AG-UI + DevMind UI (requires Full Stack first)
- **Stop E3 Full Stack** - Stops all services
- **Archon Web UI** - Opens Archon UI in browser (http://localhost:3737)

## Conversation Backup System

**ALREADY CONFIGURED** with PreCompact hook:
- Automatically backs up before conversation compaction
- Creates auto-summaries with context
- Stores in `conversation-history/`
- No manual intervention needed

## API Endpoints

When Python CSDL server is running on port 5000:
- Health: `http://localhost:5000/health`
- Models (Ollama-compatible): `http://localhost:5000/api/tags`
- Chat (Ollama-compatible): `http://localhost:5000/api/chat`
- Chat (OpenAI-compatible): `http://localhost:5000/v1/chat/completions`

## Environment Variables

Located in `.env` (gitignored):
- `CSDL_SERVER_PORT=5000`
- `CSDL_SERVER_URL=http://localhost:5000`
- `E3_AGUI_URL=http://localhost:8100`
- See `.env.example` for full list

## Git Repository

- URL: https://github.com/bodhifreeman-ux/E3
- Branch: main
- License: MIT (E3), Apache 2.0 (CSDL-14B), Proprietary (E3-DevMind-AI)

## Version Control Policy

**MANDATORY**: Commit and push changes frequently to avoid accumulating thousands of uncommitted changes.

### When to Commit:
- After completing any significant task or feature
- After fixing bugs
- After creating or modifying configuration files
- After updating documentation
- After adding new scripts or dependencies
- Before ending a work session

### Commit Message Format:
```
<type>: <short description>

Types: feat, fix, docs, style, refactor, test, chore
Example: feat: Add AG-UI server with CopilotKit integration
```

## Code Analysis Best Practices

**BEFORE making changes to existing code:**

1. **Check Git History First**
   - Run `git log --oneline -10` to see recent commits
   - Run `git diff HEAD -- <file>` to see uncommitted changes
   - This prevents recreating solutions that already exist

2. **Read README Files**
   - CSDL-14B/README.md - Model and CBP protocol
   - CSDL-ANLT/README.md - Python server setup
   - docs/E3-DEVMIND-ARCHITECTURE.md - Full architecture
   - docs/PYTORCH-BUILD-GUIDE.md - PyTorch build instructions

3. **Check Diagnostic Reports**
   - CSDL-14B/docs/CSDL-MODEL-DIAGNOSTIC.md - Known issues and solutions

## CopilotKit Integration Patterns

**Working configuration for CopilotKit 1.50.x with AG-UI Server:**

```typescript
// route.ts - Frontend API Route
import { LangGraphHttpAgent } from "@copilotkit/runtime/langgraph";

const e3DevMindAgent = new LangGraphHttpAgent({
  name: "e3_devmind",
  description: "E3 DevMind - AI-powered development assistant using CSDL-14B",
  url: process.env.E3_AGUI_URL || "http://localhost:8100",
});

const runtime = new CopilotRuntime({
  remoteEndpoints: [
    copilotKitEndpoint({
      url: AGUI_URL,
    }),
  ],
  agents: {
    e3_devmind: e3DevMindAgent,  // CRITICAL: Must register agent explicitly
  },
});
```

**Key requirements:**
- Import `LangGraphHttpAgent` from `@copilotkit/runtime/langgraph`
- Register agents explicitly in `runtime.agents`
- AG-UI server must use `LangGraphAGUIAgent` from `copilotkit` with path `/`

## Important Reminders

1. **USE PYTHON SERVER ON PORT 5000** - llama.cpp has tokenization bugs on ARM64+Blackwell
2. **Build PyTorch with CUDA** before running the CSDL server with GPU
3. **Model files are large** - SafeTensors totaling ~29.5GB
4. **32 agents, not 36** - E3-DevMind-AI has 32 agents
5. **Commit and push frequently** - Keep the git repo clean

## Troubleshooting

### Model outputs garbage/Chinese characters
- **Cause**: Using llama.cpp which has tokenization bug on ARM64+Blackwell
- **Solution**: Use Python CSDL server on port 5000 instead

### CSDL server won't start with GPU
- **Cause**: PyTorch not built with CUDA
- **Solution**: Run `./Scripts/build-pytorch-cuda.sh` (2-4 hours)

### AG-UI server shows "Agent not found"
- **Cause**: Agent not registered in CopilotKit runtime
- **Solution**: Add agent to `runtime.agents` in route.ts
