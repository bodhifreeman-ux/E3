# E3 Project Structure

```
E3/
â”œâ”€â”€ README.md                          # Project overview
â”œâ”€â”€ PROJECT-STRUCTURE.md               # This file
â”œâ”€â”€ LICENSE
â”‚
â”œâ”€â”€ CSDL-14B/                          # CSDL-14B model files (SafeTensors)
â”‚   â”œâ”€â”€ model/merged_16bit/            # 28GB FP16 model
â”‚   â””â”€â”€ docs/
â”‚       â”œâ”€â”€ 04_CSDL_14B_README.md      # Model specs
â”‚       â”œâ”€â”€ CSDL-MODEL-DIAGNOSTIC.md   # Tokenization bug investigation
â”‚       â”œâ”€â”€ CSDL_PROTOCOL.md
â”‚       â”œâ”€â”€ INSTALLATION.md
â”‚       â””â”€â”€ TRAINING.md
â”‚
â”œâ”€â”€ CSDL-ANLT/                         # CSDL server (ANLT provides translation layer)
â”‚   â”œâ”€â”€ README.md                      # CSDL server setup guide
â”‚   â”œâ”€â”€ csdl-server.py                 # CSDL inference server (FastAPI)
â”‚   â”œâ”€â”€ csdl-venv/                     # Python environment
â”‚   â””â”€â”€ docs/
â”‚       â”œâ”€â”€ 03_CSDL_ANLT_README.md     # ANLT translation layer overview
â”‚       â””â”€â”€ DGX_Spark_ANLT_Source.md
â”‚
â”œâ”€â”€ E3-DevMind-AI/                     # 32-agent cognitive swarm
â”‚   â””â”€â”€ docs/
â”‚       â”œâ”€â”€ 06_Archon_README.md
â”‚       â”œâ”€â”€ API_REFERENCE.md
â”‚       â”œâ”€â”€ DEPLOYMENT.md
â”‚       â””â”€â”€ DGX_SPARK_SETUP.md
â”‚
â”œâ”€â”€ llama.cpp/                         # llama.cpp build (has tokenization bug on ARM64)
â”‚
â”œâ”€â”€ docs/                              # Project-level documentation
â”‚   â”œâ”€â”€ 00_CLAUDE.md                   # Project context
â”‚   â”œâ”€â”€ 01_ROOT_README.md
â”‚   â”œâ”€â”€ 02_CSDL_Agent_UI_README.md
â”‚   â”œâ”€â”€ CSDL-LLAMA-SERVER-SETUP.md
â”‚   â”œâ”€â”€ DGX_Spark_CSDL_Setup.md
â”‚   â””â”€â”€ PYTORCH-BUILD-GUIDE.md
â”‚
â”œâ”€â”€ Scripts/
â”‚   â”œâ”€â”€ build-pytorch-cuda.sh          # Build PyTorch with CUDA 13.0 (requires sudo)
â”‚   â”œâ”€â”€ build-pytorch-nosudo.sh        # Build PyTorch (no sudo version)
â”‚   â”œâ”€â”€ convert-csdl-to-gguf.sh        # Convert SafeTensors to GGUF
â”‚   â””â”€â”€ start-llama-server.sh          # Start llama-server (broken on ARM64)
â”‚
â””â”€â”€ Model Files
    â””â”€â”€ csdl-14b-f16.gguf              # 28GB GGUF (for llama.cpp - broken)
```

## Current Status

âœ… **Model is functional** - fine-tuning succeeded
âŒ **llama.cpp has tokenization bug** on ARM64 + Blackwell
ğŸ”§ **Solution:** Build PyTorch with CUDA (see `build-pytorch-cuda.sh`)

## Quick Start

### Option 1: Build PyTorch (Recommended)
```bash
cd Scripts
./build-pytorch-nosudo.sh
# Wait 2-4 hours
# Then use CSDL-ANLT/csdl-server.py
```

### Option 2: Use llama.cpp from Geekom
Get working llama.cpp version from Geekom and build here.

## Documentation

- **Project context:** [docs/00_CLAUDE.md](docs/00_CLAUDE.md)
- **PyTorch build:** [docs/PYTORCH-BUILD-GUIDE.md](docs/PYTORCH-BUILD-GUIDE.md)
- **CSDL-14B diagnostics:** [CSDL-14B/docs/CSDL-MODEL-DIAGNOSTIC.md](CSDL-14B/docs/CSDL-MODEL-DIAGNOSTIC.md)
- **CSDL server setup:** [CSDL-ANLT/README.md](CSDL-ANLT/README.md)
